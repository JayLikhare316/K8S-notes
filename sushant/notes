TOPICS TO LEARN 

Core Concepts: 
Monolithic vs MicroServices, Kubernetes Architecture, Setup on Local/AWS EC2, kubectl, Pods, Namespaces, Labels ,
Selectors, Annotations 

Workloads:
Deploymnets, StatefulSets, Daemonsets, ReplicaSets, Jobs, CronJobs

Networking:
Cluster Networking, Services, Ingress, Network Policies

Storage:
persistent volume (PV), persistent volume claims (PVC), StorageClasses, ConfigMaps, Secrets 

Scaling and Scheduling:
HPA, VPA, Node Affinity, Taints and Toleration, Resource Quotas, Limit , Probes 

Cluster Administration:
RBAC, Cluster upgrade, Custom Resource Definitions (CRDs)

Monitoring and Logging:
Metrics server, Monitoring tools, Logging tools, Prometheus, Grafana, ELK Stack

Advanced Features:
Operators, Helm, Service Mesh, Kubernetes API

Security:
Pod security Standards (PSS), Image Scanning, Network Policies, Secret Encryption, Network Policies, RBAC

Cloud-Native Kubernetes:
managed Services(EKS, AKS, GKE), Cluster Autoscaler, Spot/Preemptible Nodes.

Debugging and Troubleshooting:
kubectl debugging, Logs, Resource Usage analysis

#############################################################################

Creating Ways of Kubernetes Cluster 
1.kubeadm
2.Minikube (local/ec2)
3.KIND Cluster (kubernetes in docker)
4.EKS/AKS/GKE 

#################################################################################
 
 
 ###Install and  Creating Cluster WITH KIND 
1. Create a bash file 

#!/bin/bash
[ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
chmod +x ./kind
sudo cp ./kind /usr/local/bin/kind

VERSION="v1.30.0"
URL="https://dl.k8s.io/release/${VERSION}/bin/linux/amd64/kubectl"
INSTALL_DIR="/usr/local/bin"

curl -LO "$URL"
chmod +x kubectl
sudo mv kubectl $INSTALL_DIR/
kubectl version --client

rm -f kubectl
rm -rf kind

echo "kind & kubectl installation complete."

2. chmod +x bash file
3. run the file 
4. sudo yum install docker 
5. sudo systemctl start docker && sudo systemctl enable docker 
6. sudo usermod -aG docker $USER && newgrp docker
7. mkdir kind-cluster
8.cd kind-cluster
9. vim config.yaml 

kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4

nodes:
- role: control-plane
- role: worker
- role: worker
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP

10. kind create cluster --name=demo-cluster --config=config.yaml   #---> this will create a cluster with 3 nodes
11. kubectl cluster-info --context demo-cluster      #---> to see the cluster info

###################################################################################################################

## Install and Creating Cluster With Minicube 
1. mkdir minicube-cluster
2. cd minicube-cluster
3. sudo yum install -y curl wget apt-transport-https
4. sudo yum install -y docker.io
5. sudo systemctl enable --now docker
6. sudo usermod -aG docker $USER && newgrp docker
7. curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
8. chmod +x minikube
9. sudo mv minikube /usr/local/bin/
10. curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
11. chmod +x /kubectl
12. sudo mv /kubectl /usr/local/bin/
13. minikube start --driver=docker --vm=true 
14. kubectl cluster-info --context=minikube

#########################################################################################################

### Install and creating a cluster with kubeadm

#You would need 2 instances/nodes 1.master 2. worker worker nodes can be multiple 
#EXPOSE port 6443 on security groups of EC2

1. sudo swapoff -a     #---> command on both EC2 
2.                      #---> command on both EC2
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
3.                        #---> command on both EC2
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sudo sysctl --system
lsmod | grep br_netfilter
lsmod | grep overlay
4.                                      #---> command on both EC2
sudo yum update
sudo yum install -y ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo yum update
sudo yum install -y containerd.io
containerd config default | sed -e 's/SystemdCgroup = false/SystemdCgroup = true/' -e 's/sandbox_image = "registry.k8s.io\/pause:3.6"/sandbox_image = "registry.k8s.io\/pause:3.9"/' | sudo tee /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl status containerd
5.                              #---> command on both EC2
sudo yum update
sudo yum install -y apt-transport-https ca-certificates curl gpg
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo yum update
sudo yum install -y kubelet kubeadm kubectl
sudo yum-mark hold kubelet kubeadm kubectl
6. sudo kubeadm init     #---> command on one EC2 (master node)
7.                        #---> command on one EC2 (master node)
 mkdir -p "$HOME"/.kube
 sudo cp -i /etc/kubernetes/admin.conf "$HOME"/.kube/config
 sudo chown "$(id -u)":"$(id -g)" "$HOME"/.kube/config
8.kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml    #---> command on master node
9.kubeadm token create --print-join-command #---> command on master node
####it will generate a token to join the worker nodes to the cluster copy the token 
10. sudo kubeadm reset pre-flight checks    #---> command on worker node
11. sudo kubeadm join <private-ip-of-control-plane>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash> --cri-socket 
"unix:///run/containerd/containerd.sock" --v=5        #---> command on worker node paste the token here 


###########################################################################################

container --> pod --> deployment --> service --> user 

# How to create a namespace 

1. kubectl create ns nginx-ns
2. kubectl run nginx --image=nginx -n nginx-ns    #---> create a pod through CMD in the namespace
3. kubectl get pods -n nginx-ns      #---> to see pods in the namespace
4. kubectl get pods -n nginx-ns -o wide     #---> to see pods in the namespace with more details

# how to enter in pod
1. kubectl exec -it pod/nginx-pod -n nginx-ns -- bash 
z
2.kubectl describe pod nginx-pod -n nginx-ns  #---> to see the details of the pod

## ReplicaSet/ StatefulSet /Deployment

kubectl scale deployment/nginx-deploy -n nginx-ns --replicas=5 #---> to scale the deployment

##syntax 
 kubectl scale deployment/(name-of-deployment) -n (name-of-namespace) --replicas=(number-of-replicas)

kubectl set image deployment/nginx-deployment -n nginx-ns nginx-container=nginx:1.26.3   #---> to update the image of the deployment
##syntax
kubectl set image deployment/(deploy-name) -n (namespace-name) (container-name)=image-name:tag  
kubectl rollout status deployment/nginx-deployment -n nginx-ns  #---> to check the status
kubectl rollout history deployment/nginx-deployment -n nginx-ns  #---> to check the history
kubectl rollout undo deployment/nginx-deployment -n nginx-ns  #---> to undo the last deployment
kubectl rollout undo deployment/nginx-deployment -n nginx-ns --to-revision=1  #---> to undo to a specific revision
kubectl rollout pause deployment/nginx-deployment -n nginx-ns  #---> to pause the deployment
kubectl rollout resume deployment/nginx-deployment -n nginx-ns  #---> to resume the deployment

## DaemonSets
daemonsets unsure krta hain ki har ek node me ek pod run krta rahe

##
kubectl get job      #---> to see all jobs
kubectl get job -n nginx-ns  #---> to see all jobs in a specific namespac
kubectl get job -n nginx-ns -o wide  #---> to see all jobs in a specific namespace with more details
##
kubectl logs pod/(name-of-pod-created) -n nginx-ns #---> to see the logs of the po
##
kubectl get cronjobs -n nginx-ns    #---> to see all cronjobs in a specific namespace
kubectl get cronjobs -n nginx-ns -o wide  #---> to see all cron

## Storage classes
kubectl get sc      #---> to see all storage classes
kubectl get sc -n nginx-ns  #---> to see all storage classes in a specific namespace
kubectl get sc -n nginx-ns -o wide  #---> to see all storage classes in a specific namespace with more details

#type of storage classes
1. local
2. ceph
3. csi

kubectl get pv    #---> to see all persistent volumes
kubectl get pv -n nginx-ns  #---> to see all persistent volumes in a specific namespace
kubectl get pv -n nginx-ns -o wide  #---> to see all persistent volumes in a specific namespace with more details 

kubectl get all    #---> to see all resources in the cluster
kubectl get all -n nginx-ns  #---> to see all resources in a specific namespace

kubectl get svc    #---> to see all services
kubectl get svc -n nginx-ns  #---> to see all services in a specific namespace
kubectl get svc -n nginx-ns -o wide  #---> to see all services in wide 

kubectl port-forward service/nginx-service -n nginx-ns 80:80 --address=0.0.0.0    #---> to forward the port of the service
#syntax
kubectl port-forward service/(service-name) -n (namespac) (local-port):(remote-port) --address=0.0.0.0

# What is ingress?
Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource

client -> ingress managed LB -> routing rule -> service -> pods
                                                        -> pods 


kubectl apply -f https://kind.sigs.k8s.io/examples/ingress/deploy-ingress-nginx.yaml   #---> to deploy ingress nginx
kubectl get ingress -n nginx-ns  #---> to see all ingress in a specific namespace


##Probe (request)

1.liveness probe
2.readiness probe
3.startup probe


## Taints/Tolerations

Taints: Mark a node as tainted, so that pods cannot be scheduled on it unless they have the matching tolerance
# means it marks as do not use this node

Tolerations: Allow a pod to be scheduled on a tainted node, if it has the matching tolerance
# means it allows to use this node

kubectl taint node multi-node-worker1 prod=true:NoSchedule
#Syntax
kubectl taint node (node-name) (key)=(value): (effect)    #---> to taint a node 

#effect
1. NoSchedule
2. NoExecute
3. PreferNoSchedule

kubectl untaint node multi-node-worker1
#Syntax
kubectl untaint node (node-name)        #---> to untaint a node

kubectl get nodes -o wide      #---> to check which nodes are tainted

##Types of auto scalling
1.HPA (Horizontal pod auto scalling)   #---> to scale the number of replicas of a pod
2.VPA (Vertical pod auto scalling)   #---> to scale the resources of a pod
3.KEDA (Kubernetes event-driven autoscaling)    #---> to scale the number of replicas of a pod based on 
#the number of events

#HPA
kubectl autoscale deployment my-deployment --min=1 --max=10 --cpu-percent=50
#Syntax
kubectl autoscale deployment (deployment-name) --min=(min-replicas) --max=(max-replicas) --cpu-percent=(cpu-percent)    #---> to create a HPA
kubectl delete hpa my-hpa         #---> to delete a HPA
kubectl get hpa -o wide         #---> to get all HPA
kubectl get hpa my-hpa -o yaml    #---> to get the status of a HPA

#VPA
kubectl create vpa --namespace default --name my-vpa --target-references my-deployment
#Syntax
kubectl create vpa --namespace (namespace) --name (vpa-name) --target-references
#(target-deployment)    #---> to create a VPA
kubectl delete vpa my-vpa         #---> to delete a VPA
kubectl get vpa -o wide         #---> to get all VPA
kubectl get vpa my-vpa -o yaml    #---> to get the status of a VPA

#KEDA
kubectl create keda --namespace default --name my-keda --target-references my-deployment
#Syntax
kubectl create keda --namespace (namespace) --name (keda-name) --target-references (target-deployment)   #---> to create a KEDA

#what is Metrics
Metrics are the data that is collected from the cluster, such as CPU usage, memory usage, and
network traffic. This data is used to make decisions about the cluster, such as scaling up or down
or identifying performance issues.

##install metric-server on a kind-cluster
1. kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

2.kubectl -n kube-system edit deployment metrics-server
3. ##add this in container in args
- --kubelet-insecure-tls
- --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP

4. kubectl -n kube-system rollout restart deployment metrics-server
5.
kubectl get pods -n kube-system
kubectl top nodes



kubectl run -i --tty load-generator --image=busybox -n apache -- /bin/sh    #---> to run a pod in interactive mode
#syntax
kubectl run -i --tty (pod-name) --image (image-name) -n (namespace) -- (command/shelltype)        #--> to run a pod in interactive mode

while true; do wget -q -O- http://load-generator.default.svc.cluster.local; done      #----> to run on loop / stress app
#syntax
while true; do (command) ; done      #---> to run on loop / stress app

##For VPA
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh

# What is Node affinity
Node affinity is a way to specify which nodes a pod can run on. It is used to ensure 
that a pod runs on a node that has the required resources and meets the specified requirements.

# RBAC (Role -Based Access Control)
RBAC is a way to control access to resources in a cluster. It is used to ensure that
users and groups have the correct permissions to perform actions on resources.

Namespace: Roles, RoleBinding
Cluster: ClusterRole, ClusterRoleBinding
# RBAC
kubectl create clusterrolebinding my-cluster-role-binding --clusterrole=my-cluster-role --user=my-user
#syntax
kubectl create clusterrolebinding (cluster-role-binding-name) --clusterrole=(cluster-role-name) --user=(user-name)      #---> to create a cluster role binding
# RBAC


1. Service Account:
kubectl get sa          #--> to get all service accounts
kubectl get sa -o yaml  #--> to get all service accounts in yaml format
kubectl get sa -o wide  #--> to get all service accounts in wide format
2. Roles
kubectl get roles      #--> to get all roles
kubectl get roles -o yaml  #--> to get all roles in yaml format
kubectl get roles -o wide  #--> to get all roles in wide format
kubectl create role my-role --verb=get --verb=list --verb=watch --resource=pods --namespace=my-namespace
#syntax
kubectl create role (role-name) --verb=(verb) --resource=(resource)      #---> to create a role
3. Role Bindings
kubectl get rolebindings      #--> to get all role bindings
kubectl get rolebindings -o yaml  #--> to get all role bindings in yaml format
kubectl get rolebindings -o wide  #--> to get all role bindings in wide format
kubectl create rolebinding my-role-binding --role=my-role --user=my-user --namespace=my-namespace
#syntax
kubectl create rolebinding (role-binding-name) --role=(role-name) --user=(user-name) --namespace=(namespace-name)      #---> to create a role binding
4. Cluster Roles
kubectl get clusterroles      #--> to get all cluster roles
kubectl get clusterroles -o yaml  #--> to get all cluster roles in yaml format
kubectl get clusterroles -o wide  #--> to get all cluster roles in wide format
kubectl create clusterrole my-cluster-role --verb=get --verb=list --verb=watch --resource=pods 
#syntax
kubectl create clusterrole (cluster-role-name) --verb=(verb) --resource=(resource) #---> to create a cluster role
5. Cluster Role Bindings
kubectl get clusterrolebindings      #--> to get all cluster role bindings
kubectl get clusterrolebindings -o yaml  #--> to get all cluster role bindings in yaml format
kubectl get clusterrolebindings -o wide  #--> to get all cluster role bindings in wide format
kubectl create clusterrolebinding my-cluster-role-binding --clusterrole=my-cluster-role --user=my-user
#syntax
kubectl create clusterrolebinding (cluster-role-binding-name) --clusterrole=(cluster-role-name) --user=(user-name)   #---> to create a cluster role binding

kubectl auth whoami    #---> to get the current user

kubectl apply -f .   #---> to apply the configuration in the current directory/folder all included YAML files will be run
kubectl delete -f .   #---> to delete the configuration in the current directory/folder all included YAML files will be run

kubectl can-i get pods --as=my-user  #---> to check if the user has the permission to get
#syntax
kubectl can-i (verb) (resource) --as=(user-name)      #---> to check if the user has the permission to perform the action

apiGroups:
- apiextensions.k8s.io    #---> used in 
- apps                #---> used in Deployment version
- authentication.k8s.io  #---> used in 
- autoscaling            #---> used in autoscliing version
- batch          #used in 
- certificates.k8s.io    #---> used in 
- core
- extensions
- networking.k8s.io
- policy
- rbac.authorization.k8s.io   #---> used in RBAC Yaml version
- storage.k8s.io

Resources:
- configmaps
- cronjobs
- deployments
- events
- jobs
- limitranges
- namespaces
- nodes
- persistentvolumeclaims
- persistentvolumes
- pods
- replicationcontrollers
- resourcequotas
- secrets
- serviceaccounts
- services
- storageclasses
- volumes

Verbs:
- create
- delete
- get
- list
- patch
- read
- update






